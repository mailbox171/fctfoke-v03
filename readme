---------------------------------------------------------------------------
FC ORACLE OCI OKE Sample Architecture
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Setting up with Terraform
---------------------------------------------------------------------------

From Terraform point of view, we have two COMPONENTS
100-fr
200-core 

The are arrenged in a LAYERED ARCHITECTURE, PLEASE CREATE AND DESTROY IN REVERSE ORDER
   Create      100-fr   -->  200-core 
   Destroy     200-core -->  100-fr     



Terraform VARIABLE SCOPES
-------------------------
Roughly each VCN is a component.
Variables for each component are generally defined (variables.tf) and assigned (terraform.tfvars) in the component root directory.

In addition, the following scopes can be used (values will OVERRIDE what is in the root directory).

# GLOBAL                              -var-file=./../g.tfvars
# COMPONENT-ENVIRONMENT-REGION        -var-file=./$TFENV/$TFREGION/cer.tfvars
# COMPONENT-ENVIRONMENT               -var-file=./$TFENV/ce.tfvars
# ENVIRONMENT                         -var-file=./../vars/envs/$TFENV/e.tfvars
# REGION                              -var-file=./../vars/regions/$TFREGION/r.tfvars




SETUP STEPS
-----------

export TFENV=dev
export TFREGION=eu-frankfurt-1

vi ~/.bashrc
######Add these lines
alias tinit="terraform init -var-file=./../g.tfvars -var-file=./$TFENV/$TFREGION/cer.tfvars -var-file=./$TFENV/ce.tfvars -var-file=./../vars/envs/$TFENV/e.tfvars -var-file=./../vars/regions/$TFREGION/r.tfvars"
alias tplan="terraform plan -var-file=./../g.tfvars -var-file=./$TFENV/$TFREGION/cer.tfvars -var-file=./$TFENV/ce.tfvars -var-file=./../vars/envs/$TFENV/e.tfvars -var-file=./../vars/regions/$TFREGION/r.tfvars"
alias tapply="terraform apply  -var-file=./../g.tfvars -var-file=./$TFENV/$TFREGION/cer.tfvars -var-file=./$TFENV/ce.tfvars -var-file=./../vars/envs/$TFENV/e.tfvars -var-file=./../vars/regions/$TFREGION/r.tfvars"
 
source ~/.bashrc


MODULE 100 FR SETUP - RUN WORDPRESS AGAINST MYSQL
-------------------------------------------------

cd 100-fr

edit sec.auto.tfvars
(set variables values)

export TFENV=dev
export TFREGION=eu-frankfurt-1

source  ~/.bashrc   # ALWAYS!





tinit
tplan
tapply

Apply complete! Resources: 31 added, 0 changed, 0 destroyed.
Outputs:
(..)
ssh_to_operator = "ssh -i ~/keys/ssh-key-2021-07-01.key -J opc@xxx.yyy.227.241 opc@zzz.www.0.6"




SSH TO OPERATOR
Insert "-o StrictHostKeyChecking=no" option 

ssh_to_operator = "ssh -o StrictHostKeyChecking=no  -i ~/keys/ssh-key-2021-07-01.key -J opc@130.61.227.241 opc@10.0.0.6"
(..)
Are you sure you want to continue connecting (yes/no)? yes


TEST KUBECTL CONNECTION
[opc@dev-operator ~]$ kubectl get nodes

NAME           STATUS   ROLES   AGE     VERSION
10.0.114.244   Ready    node    6m47s   v1.19.7


CREATE MYSQL DB
sudo yum install mysql-shell
#mysqlsh Username@IPAddressOfMySQLDBSystemEndpoint
mysqlsh adminUser@10.0.3.8
BEstrO0ng_#11
 
\sql CREATE DATABASE polls;
Query OK, 1 row affected (0.0038 sec)

\quit



CLONE THIS REPO ON OPERATOR
git clone https://github.com/mailbox171/<repo-name>



GO TO K8S WORDPRESS K8S MANIFEST FOLDER 
cd fctfoke-v01/100-fr/k8s/wp/

APPLY MANIFEST .yaml FILES
[opc@dev-operator wp]$ kubectl apply -f svc-mysql.yaml 
service/external-mysql-service created
endpoints/external-mysql-service created

[opc@dev-operator wp]$ kubectl apply -f wp.yaml 
service/wordpress created
persistentvolumeclaim/wp-pv-claim created
deployment.apps/wordpress created




CHECK SERVICES, WAIT FOR EXTERNAL ADDRESS (may be 'pending' for a while)
[opc@dev-operator wp]$ kubectl get svc
NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)        AGE
external-mysql-service   ClusterIP      10.96.104.164   <none>           3306/TCP       81s
kubernetes               ClusterIP      10.96.0.1       <none>           443/TCP        29m
wordpress                LoadBalancer   10.96.192.55    129.159.243.47   80:30952/TCP   40s




GO TO ADDRESS, IN ANY INTERNET BROWSER
http://129.159.243.47

WordPress website should be reached


REMINDER
When destroyng this component, delete load balancer in k8s, before terraform destroy.
If you frget, you can also delete the LB using OCI console

[opc@dev-operator wp]$ kubectl kubectl delete service wordpress 





Set-up NGINX Ingress Controller
-------------------------------

Ingress Controller 
The ingress controller comprises:

- An ingress controller deployment called nginx-ingress-controller. 
  The deployment deploys an image that contains the binary for the ingress controller and Nginx. 
  The binary manipulates and reloads the /etc/nginx/nginx.conf configuration file when an ingress is created in Kubernetes.
  Nginx upstreams point to services that match specified selectors.
- An ingress controller service called ingress-nginx. 
  The service exposes the ingress controller deployment as a LoadBalancer type service. 
  Because Container Engine for Kubernetes uses an Oracle Cloud Infrastructure integration/cloud-provider, 
  a load balancer will be dynamically created with the correct nodes configured as a backend set.

Backend Components
The hello-world backend comprises:

- A backend deployment called docker-hello-world. This is done by using a stock hello-world image that serves the minimum required routes for a default backend.
- A backend service called docker-hello-world-svc.The service exposes the backend deployment for consumption by the ingress controller deployment.



Setting Up the Example Ingress Controller
1. If you haven't already done so, follow the steps to set up the cluster's kubeconfig configuration file
                 export KUBECONFIG=<repo-root>/100-fr/generated/kubeconfig


Creating the Service Account, and the Ingress Controller
1. Run the following command to create the nginx-ingress-controller ingress controller deployment, along with the Kubernetes RBAC roles and bindings:
                 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/cloud/deploy.yaml

To check if the ingress controller pods have started, run the following command:

                 kubectl get pods -n ingress-nginx \
                      -l app.kubernetes.io/name=ingress-nginx --watch

To detect which version of the ingress controller is running, exec into the pod and run nginx-ingress-controller --version.

                 POD_NAMESPACE=ingress-nginx
                 POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
                 kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version

Create the ingress-nginx ingress controller service by running the following command:
                cd <repo-root>/100-fr/k8s/ingress
                kubectl apply -f cloud-generic.yaml

Verify that the ingress-nginx Ingress Controller Service is Running as a Load Balancer Service. View the list of running services by entering:
                kubectl get svc -n ingress-nginx   
The output from the above command shows the EXTERNAL-IP for the ingress-nginx Service.


Creating a TLS Secret.
A TLS secret is used for SSL termination on the ingress controller. Output a new key to a file. For example, by entering:
                openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/CN=nginxsvc/O=nginxsvc"

Create the TLS secret by entering: 
                kubectl create secret tls tls-secret --key tls.key --cert tls.crt



Setting Up the Example Backend. In this section, you define a hello-world backend service and deployment.
Create the new hello-world deployment and service on nodes in the cluster by running the following command:
                kubectl create -f hello-world-ingress.yaml


Using the Example Ingress Controller to Access the Example Backend
In this section you create an ingress to access the backend using the ingress controller.
                kubectl create -f ingress.yaml

Verify that the Example Components are Working as Expected.
To confirm the ingress-nginx service is running as a LoadBalancer service, obtain its external IP address by entering:
                kubectl get svc --all-namespaces


Sending cURL Requests to the Load Balancer
Use the external IP address of the ingress-nginx service (for example, 129.146.214.219) to open a browser, or otherwise curl an http request by entering:
                curl --trace -  http://<EXTERNALIP>



One more Ingress example
------------------------
(from https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/ )

Create a Deployment using the following command:
                kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0

Expose as service
                kubectl expose deployment web --type=NodePort --port=8080

Create ingress resource
                kubectl apply -f example-ingress.yaml

Sending cURL Requests to the Load Balancer (Hello world)
Use the same external IP address (adding the path /helloworld) of the ingress-nginx service (for example, 129.146.214.219) to open a browser, or otherwise curl an http request by entering:
                curl --trace -  http://<EXTERNALIP>/helloworld

Check the / path still works (Hello webhook world)
Use the external IP address of the ingress-nginx service (for example, 129.146.214.219) to open a browser, or otherwise curl an http request by entering:
                curl --trace -  http://<EXTERNALIP>


 






---------------------------------- MODULE 200 CORE  ---------------------------------------------------------
These are the steps to create the second layer, with a different VCN, and Local Peering Gateway with the first,
and its own OKE cluster

cd ..
cd 200-core

edit sec.auto.tfvars
(set variables values)



# not necessary, if already done
export TFENV=dev
export TFREGION=eu-frankfurt-1

source  ~/.bashrc   # ALWAYS!





tinit
tplan
tapply



------------------------ CLEAN UP -------------------------------
-----------------------------------------------------------------
>>>> PLEASE DESTROY IN REVERSE ORDER
200-core
100-fr 


The steps to clean up are:

             Go to repo root directory
             Repeat initial Terraform setup, if becessary (env variables and init script sourcing)

             cd 200-core
             tdestroy

             cd ../100-fr
             tdestroy









